{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3393916a",
   "metadata": {},
   "source": [
    "# Simple two-layer neural network to classify documents into three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies, set a random set, and define the dataset\n",
    "\n",
    "import re, torch, torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "docs = [\n",
    "    \"Movies are fun for everyone\",\n",
    "    \"Watching movies is great fun\",\n",
    "    \"Enjoy a great movie tonight.\",\n",
    "    \"Research is interesting and important\",\n",
    "    \"Learning math is very important\",\n",
    "    \"Science discovery is interesting\",\n",
    "    \"Rock is great to listen to\",\n",
    "    \"Listen to music for fun\",\n",
    "    \"Music is fun for everyone.\",\n",
    "    \"Listen to folk music!.\"\n",
    "]\n",
    "\n",
    "labels = [1,1,1,3,3,3,2,2,2,2]\n",
    "num_classes = len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef040fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents to a bag of words using tokenize and get_vocabulary\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "def get_vocabulary(texts):\n",
    "    tokens = {token for text in texts for token in tokenize(text)}\n",
    "    return {word: idx for idx, word in enumerate(sorted(tokens))}\n",
    "\n",
    "vocabulary = get_vocabulary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c987b09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'and': 1,\n",
       " 'are': 2,\n",
       " 'discovery': 3,\n",
       " 'enjoy': 4,\n",
       " 'everyone': 5,\n",
       " 'folk': 6,\n",
       " 'for': 7,\n",
       " 'fun': 8,\n",
       " 'great': 9,\n",
       " 'important': 10,\n",
       " 'interesting': 11,\n",
       " 'is': 12,\n",
       " 'learning': 13,\n",
       " 'listen': 14,\n",
       " 'math': 15,\n",
       " 'movie': 16,\n",
       " 'movies': 17,\n",
       " 'music': 18,\n",
       " 'research': 19,\n",
       " 'rock': 20,\n",
       " 'science': 21,\n",
       " 'to': 22,\n",
       " 'tonight': 23,\n",
       " 'very': 24,\n",
       " 'watching': 25}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11fba1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction function that converts a document into a feature vector\n",
    "\n",
    "def doc_to_bow(doc, vocabulary):\n",
    "    tokens = set(tokenize(doc))\n",
    "    bow = [0]*len(vocabulary)\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            bow[vocabulary[token]] = 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baceaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform documents and labels into numbers\n",
    "vectors = torch.tensor(\n",
    "    [doc_to_bow(doc, vocabulary) for doc in docs],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "labels = torch.tensor(labels, dtype=torch.long) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5bf60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors: \n",
      "tensor([[0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "\n",
      "labels: \n",
      "tensor([0, 0, 0, 2, 2, 2, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"vectors: \\n{}\\n\\nlabels: \\n{}\".format(vectors, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
